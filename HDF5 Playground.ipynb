{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook I will be reading the book ['Python and HDF5, Unlocking Scientific Data'](http://shop.oreilly.com/product/0636920030249.do) and try implementing the code given in the book, either as is or modified to suit my understanding or try some new thing out. The notebook will be filled with a lot of code accompanied by comments mentioning some important concept, intuition. To finish the book faster and get as much content as possible in the notebook I won't be giving the background for the HDF5 file format and other high level theoretical details which can be found in the book or other pages online but focus more on practical aspect of coding in Python and using HDF5 for storing scientific data.\n",
    "\n",
    "The HDF5 web site can be found [here](https://support.hdfgroup.org/HDF5/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the an example of data collected from weather station. Suppose we have 10 weather stations numbered from 1 to 10 for a date 1-Jan-2017 and each of them record temperature in Fahrenheit and wind speed in mph. We assume the numbers are integers and not floating point numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = 1\n",
    "month = 'Jan'\n",
    "year = 2017\n",
    "\n",
    "station_ids = range(1, 11, 1)\n",
    "np.random.seed(0)\n",
    "temperatures = np.asarray([np.random.randint(60, 80) for _ in station_ids])\n",
    "wind_speeds = np.asarray([np.random.randint(0, 10) for _ in station_ids])\n",
    "\n",
    "with h5py.File('weather.hdf5', 'w') as f:\n",
    "    for station_id, temperature, wind_speed in zip(station_ids, temperatures, wind_speeds):        \n",
    "        temperature_key = '/' + str(station_id) + '/temperature'\n",
    "        f[temperature_key] = temperature\n",
    "        f[temperature_key].attrs['date'] = 1\n",
    "        f[temperature_key].attrs['month'] = month\n",
    "        f[temperature_key].attrs['year'] = year\n",
    "        wind_speed_key = '/' + str(station_id) + '/wind_speed'\n",
    "        f[wind_speed_key] = wind_speed\n",
    "        f[wind_speed_key].attrs['date'] = 1\n",
    "        f[wind_speed_key].attrs['month'] = month\n",
    "        f[wind_speed_key].attrs['year'] = year\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now open this file and retrieve readings of the station 7 and confirm its what we wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature at station 7 is 69 ,wind speed recorded is 7 , the date these measurements were taken is 1-Jan-2017\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('weather.hdf5') as f:\n",
    "    station7_temperature = f['/7/temperature']\n",
    "    station7_wind_speed = f['/7/wind_speed']\n",
    "    assert station7_temperature.value == temperatures[6], 'Value not same as the one written'\n",
    "    assert station7_wind_speed.value == wind_speeds[6], 'Value not same as the one written'\n",
    "    temperature_node_attrs = dict([a for a in station7_temperature.attrs.items()])\n",
    "    print('Temperature at station 7 is', station7_temperature.value, ',wind speed recorded is'\n",
    "          ,station7_wind_speed.value, ', the date these measurements were taken is',\n",
    "         '%d-%s-%d'%(temperature_node_attrs['date'], temperature_node_attrs['month'], temperature_node_attrs['year']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 file is not entirely loaded in memory but only the data required and read is loaded. In above case the weathers file may have a lot of data but only the necessary information about station 7 was read in memory when requested\n",
    "\n",
    "Let's look at another example. where we create a dataset (we are yet to see what a dataset is).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the file BigArrayFile.hdf5 is 1400 bytes\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('BigArrayFile.hdf5', 'w') as f:\n",
    "    dataset = f.create_dataset('big', shape = (1024, 1024), dtype = 'float32')\n",
    "    \n",
    "stats = os.stat('BigArrayFile.hdf5')\n",
    "print('Size of the file BigArrayFile.hdf5 is',stats.st_size, 'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, we created an HDF5 file and created a data set called big in it of shape $1024 \\times 1024$ of type float32. Yet, the size of the file on the disk is 1400 bytes, let us set a byte at index (2, 2) with value 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the file BigArrayFile.hdf5 is 4195704 bytes\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('BigArrayFile.hdf5') as f:\n",
    "    dataset = f['big']\n",
    "    dataset[2, 2] = 2.0\n",
    "    \n",
    "stats = os.stat('BigArrayFile.hdf5')\n",
    "print('Size of the file BigArrayFile.hdf5 is',stats.st_size, 'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we see above, once we accessed the byte of the data set the entire dataset was flushed to the disk. The shape times 4 bytes(for float32) per location should take $1025 \\times 1024 \\times 8 = 4194304$, the size we see above is pretty close to this number as there HDF5 itself takes few bytes for the meta data. Also, an interesting point to note is that the dataset can be large in size (large enough to load all in memory), but only the bytes accessed will be loaded in memory.\n",
    " \n",
    " HDF5 also supports compression of the data. Lets create a dataset of same size $1024 \\times 1024$, but create the dataset using compression gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the file BigCompressedArrayFile.hdf5 is 4075 bytes\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('BigCompressedArrayFile.hdf5', 'w') as f:\n",
    "    dataset = f.create_dataset('big', shape = (1024, 1024), dtype = 'float32', compression = 'gzip')\n",
    "    dataset[2, 2] = 2.0\n",
    "    \n",
    "stats = os.stat('BigCompressedArrayFile.hdf5')\n",
    "print('Size of the file BigCompressedArrayFile.hdf5 is',stats.st_size, 'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, the file containing the dataset with same shape, but compressed has a much lower size of the data stored on the disk. There is however a tradeoff between space on disk and CPU time required to compress and decompress the contents. We will talk more on this later in the notebook.\n",
    "\n",
    "We have used ``h5py.File`` to open the file. The second parameter is the ``mode`` argument which can either be\n",
    "\n",
    "* r  : read only for existing file, fails when the provided file is not present\n",
    "* r+ : read/write for existing file, fails when the provided file is not present\n",
    "* w  : write, create a new file, truncates an existing file\n",
    "* w- : write, same as w except that it doesnt truncate an existing file but the operation fails\n",
    "* a  : read/write, if existing file not found, new one will be created (this if not the same in case of r+, but same as using either w or r+ depending on whether the file exists of not)\n",
    "\n",
    "---\n",
    "\n",
    "#### Drivers\n",
    "\n",
    "HDF5 drivers are the ones who map the bytes to be written to the low level disk bytes. Following are some important types of drivers\n",
    "\n",
    "* SEC2: Default driver that uses posix file system read/write functions to a single file\n",
    "* STDIO: Use the stdio.h for performing buffered read and write functions to single file\n",
    "* CORE: Performs read/write in memory for with option an option to create backing files on file system.\n",
    "* FAMILY: Partitions a large file in multiple chunks\n",
    "* MPIIO: Parallel HDF5 which allows multiple processes to read/write from same HDF5 file in parallel. We will see this type of driver in more details later.\n",
    "\n",
    "---\n",
    "\n",
    "### Datasets\n",
    "\n",
    "In this section we will look at datasets in HDF5\n",
    "\n",
    "Datasets are like Numpy array on disk. These datasets have a name, shape, type and can be sliced like an in memory Numpy array except that the entire dataset need not be loaded to memory and only the accessed slices from disk are loaded in memory as needed.\n",
    "\n",
    "Let us now create a Numpy array and create a data set out of it in an HDF5 file. Lets get some attributes of the dataset and the numpy array and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array type is int64 shape is (1, 5) type of arr is <class 'numpy.ndarray'>\n",
      "Dataset type is int64 shape is (1, 5) type of a is <class 'h5py._hl.dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3, 4, 5]])\n",
    "print('Numpy array type is', arr.dtype, 'shape is', arr.shape, 'type of arr is', type(arr))\n",
    "\n",
    "with h5py.File('DatasetTest.hdf5', 'w') as f:\n",
    "    f['arr'] = arr\n",
    "\n",
    "# Opening the file again, in read mode\n",
    "with h5py.File('DatasetTest.hdf5', 'r') as f:\n",
    "    a = f['arr']\n",
    "    print('Dataset type is', a.dtype, 'shape is', a.shape, 'type of a is', type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can see the similarity between Numpy and h5py API.\n",
    "\n",
    "Lets now read, modify the dataset and slice the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Arr type is int64 shape is (1, 5) type of a is <class 'numpy.ndarray'>\n",
      "2. Arr now is [[10 10 10 10 10]] Dataset is [[1 2 3 4 5]]\n",
      "3. Arr type is int64 shape is (2,) type is <class 'numpy.ndarray'> contents are [3 4]\n",
      "4. Dataset now is [[10 10 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('DatasetTest.hdf5', 'a') as f:\n",
    "    #1\n",
    "    arr = f['arr'][:]\n",
    "    print('1. Arr type is', arr.dtype, 'shape is', arr.shape, 'type of a is', type(arr))\n",
    "    \n",
    "    #2\n",
    "    arr[:] = 10\n",
    "    print('2. Arr now is', arr[:], 'Dataset is', f['arr'][:])\n",
    "    \n",
    "    #3\n",
    "    arr = f['arr'][0, 2:4]\n",
    "    print('3. Arr type is', arr.dtype, 'shape is', arr.shape, 'type is', type(arr), 'contents are', arr)\n",
    "    \n",
    "    #4\n",
    "    f['arr'][:] = 10\n",
    "    print('4. Dataset now is', f['arr'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have 4 outputs on the previous line and following are the observations\n",
    "\n",
    "1. We can read the entire dataset in memory using `:` on the dataset object of the HDF5 file and serialize the contents to a numpy array in memory. Be careful while performing this operation on a large dataset as the entire dataset is seldom needed top be loaded in memory or in the worst case the size may be much larger than the available memory. The `...` can be used instead of `:` to reference the entire dataset.\n",
    "2. Once in memory, this numpy array is just a copy and stale copy of the dataset in the file.  There is more synchronization between the array and the dataset in the file. As we see, any changes made to the contents of the array are local to the array and not reflected if we read the contents of the file back.\n",
    "3. The access here to the dataset is how we will be practically accessing the HDF5 dataset in the file. We slice a part of it as we need to read and only and only this slice is serialized to memory by the HDF5 library.\n",
    "4. Here we modify the entire dataset in the file. We would seldom do this in a real application especially for large datasets but only a slice of the dataset will be updated.\n",
    "\n",
    "---\n",
    "\n",
    "We will look at the how we can control the datatypes of the data that is read and written from and to the underlying HDF5 file.\n",
    "\n",
    "Suppose we have a numpy array with double precision datatype float64 but we are ok to store the data as float32 to save some extra disk space effectively making the IO fast.\n",
    "\n",
    "Let us first write the same array of double precision to the underlying file, one using double precision and one using single precision floating point type and compare the size of the files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of file holding float64 array is 82144 , Size of file holding float32 is 42144\n"
     ]
    }
   ],
   "source": [
    "arr = np.random.rand(100, 100)\n",
    "\n",
    "with h5py.File('DoublePrecision.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset('arr', data = arr, dtype = 'float64')\n",
    "    \n",
    "with h5py.File('SinglePrecision.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset('arr', data = arr, dtype = 'float32')\n",
    "    \n",
    "dblstat = os.stat('DoublePrecision.hdf5')\n",
    "singlestat = os.stat('SinglePrecision.hdf5')\n",
    "\n",
    "print('Size of file holding float64 array is', dblstat.st_size, \n",
    "      ', Size of file holding float32 is', singlestat.st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above, the size of the file is halved by changing the datatype. This is a good optimization provided that the change of double to single precision float doesn't impact the application using the data. Similar choices can be made when using integer datatype in choosing between 8, 16 and 32 bits depending on the expected range of the values the data can take. This is no different than choosing datatypes for a variable in programming languages\n",
    "\n",
    "Let's say we have written the float64 numpy array as a float32 array. This means when we read the value back, the datatype of the numpy array will be float32 which might not be desirable and we would want to seamlessly read the value into a float64 numpy array. There are a couple of ways we can do that as we see below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of arr is float32 dtype of arr is float64 dtype of arr2 is float64\n",
      "Equality check of arr and arr1 gives True\n",
      "Equality check of arr1 and arr2 gives True\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('SinglePrecision.hdf5', 'r') as f:\n",
    "    #1\n",
    "    ds = f['arr']\n",
    "    arr = ds[:]\n",
    "    #2\n",
    "    arr1 = np.empty(shape = ds.shape, dtype = 'float64')\n",
    "    ds.read_direct(arr1)\n",
    "    #3\n",
    "    with ds.astype('float64'):\n",
    "        arr2 = ds[:]\n",
    "\n",
    "print('dtype of arr is', arr.dtype, 'dtype of arr is', arr1.dtype, 'dtype of arr2 is', arr2.dtype)\n",
    "print('Equality check of arr and arr1 gives',(arr == arr1).all())\n",
    "print('Equality check of arr1 and arr2 gives',(arr2 == arr1).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see from the above output, by default, the array is read as a float32 and the datatype of the original array written is lost. If we want to read the object back the object as a float64 array, the first approach (given in #2 above) is to initialize an empty numpy array of the desired target type and pass it to ``read_direct`` method of the HDF5 dataset instance. Another alternate(and cleaner in my opinion) is given in #3\n",
    "\n",
    "---\n",
    "\n",
    "Let us see the default values of an empty dataset and see how to use a different default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Using Defaults: arri,  [[0 0]] arrf,  [[ 0.  0.]]\n",
      "2. Using FillValues: arri,  [[-1 -1]] arrf,  [[ nan  nan]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('FillValue.hdf5', 'w') as f:\n",
    "    dseti = f.create_dataset('empty_ints', shape = (1, 2), dtype = 'int16')\n",
    "    dsetf = f.create_dataset('empty_floats', shape = (1, 2), dtype = 'float32')\n",
    "    arri = dseti[:]\n",
    "    arrf = dsetf[:]\n",
    "    print('1. Using Defaults: arri, ', arri, 'arrf, ', arrf)\n",
    "    dseti = f.create_dataset('fillv_ints', shape = (1, 2), dtype = 'int16', fillvalue = -1)\n",
    "    dsetf = f.create_dataset('fillv_floats', shape = (1, 2), dtype = 'float32', fillvalue = float('nan'))\n",
    "    arri = dseti[:]\n",
    "    arrf = dsetf[:]\n",
    "    print('2. Using FillValues: arri, ', arri, 'arrf, ', arrf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above, the default value for int as well as float is 0. The domain of the application might have 0 as a valid value and thus we desire to different value which clearly identifies the value at that location in the array being absent and not a valid value. In such case we can use the ``fillvalue`` parameter to use a different fill value other than 0.\n",
    "\n",
    "Suppose our application domain requires a value to be a non negative value for int fields and a non Nan value to be used for floating point numbers, it makes sense to give a negative value as a fill value for integers and a NaN for floats as a default value.\n",
    "\n",
    "---\n",
    "\n",
    "### Reading and writing Data\n",
    "\n",
    "Let us create a dataset of size $100 \\times 1000 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"arr1\": shape (100, 1000), type \"<f8\">\n",
      "Slice shape (10, 50) , Slice type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "arr = np.random.randn(100, 1000)\n",
    "with h5py.File('Slicing.hdf5', 'w') as f:\n",
    "    f['arr1'] = arr\n",
    "    f['arr2'] = arr\n",
    "    dset = f['arr1']\n",
    "    #1\n",
    "    print(dset)\n",
    "    slice = dset[10:20, 20:70]\n",
    "#2\n",
    "print('Slice shape', slice.shape, ', Slice type', type(slice))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above, we have two datasets of size $100 \\times 1000$\n",
    "If we want a slice of it of ``dset[10:20, 20:70]`` we get a ``numpy.ndarray`` of shape `(10, 50)` and following is what happenes when we request the slice\n",
    "\n",
    "1. The shape of the data is figured out by h5py, in this case ``(10, 50)``\n",
    "2. An empty numpy array is instantiated of this size\n",
    "3. Appropriate portions of the data set are selected, this may include the necessary IO operations to take place on the underlying HDF5 file.\n",
    "4. Copy the data read to the empty numpy array instantiated\n",
    "5. Return the numpy array\n",
    "\n",
    "All these operations are performed before accessing any portion of the dataset on the HDF5 file. It is thus highly recommended to vectorize the operations as far as possible. Following example demonstrates vectorzation.\n",
    "\n",
    "Suppose we want to chop the values of the dataset with values less than 0 to 0. We have the following two ways\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 2.86 s, total: 33.9 s\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Non Vectorized\n",
    "with h5py.File('Slicing.hdf5', 'a') as f:\n",
    "    dset1 = f['arr1']\n",
    "    \n",
    "    for ix in range(100):\n",
    "        for iy in range(1000):\n",
    "            val = dset1[ix, iy]\n",
    "            if val < 0: dset1[ix, iy] = 0\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61.2 ms, sys: 6.92 ms, total: 68.1 ms\n",
      "Wall time: 67.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Vectorized\n",
    "with h5py.File('Slicing.hdf5', 'a') as f:\n",
    "    dset2 = f['arr2']    \n",
    "    \n",
    "    for ix in range(100):\n",
    "        val = dset2[ix, :]\n",
    "        val[val < 0] = 0\n",
    "        dset2[ix,:] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above there is a signficant performance difference in updating the matrix element by element in a HDF5 dataset vs vectoring the operations along the longer dimension. Note that there is nothing stopping us from loading the entire data set in memory for the matrix of $100 \\times 1000$ case above, but for real world datasets we won't be loading the entire dataset in memory for manipulation, instead we will be reading the large chunk of data in memory and vectorise the operation on along these slices in a loop. It is interesting to see how the operation using slicing effectively scales up with such a small dataset. You can imagine the performance benefits on the real world, large dataset containing millions of elements\n",
    "\n",
    "Finally we want to ensure that two matrices contain identical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two data sets equality check gives True\n",
      "Negative number check gives True\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Slicing.hdf5', 'r') as f:\n",
    "    dset1 = f['arr1'][:]\n",
    "    dset2 = f['arr2'][:]\n",
    "    print('Two data sets equality check gives', (dset1 == dset2).all())\n",
    "    print('Negative number check gives', (dset1 >= 0).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You might have noticed that we use indexing of datasets in HDF5 is similar to Numpy datasets. Following snippet gives some ways to access the dataset elements. \n",
    "\n",
    "An important thing to remember is we **cannot** read the dataset in reverse order line a Numpy array\n",
    "by giving the range as ``-1:0`` also the step value has to be a non negative number where the format of the array index is ``[start]:[end]:[step]``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Third element is 3\n",
      "2. Last element is 9\n",
      "3. Read elements 2 thru 7 [2 3 4 5 6]\n",
      "4. Read elements 4 thru 8, step by 2 [4 6]\n",
      "5. Read elements 4 to end of the array [4 5 6 7 8 9]\n",
      "6. Read elements 4 to second to last element [4 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Indexing.hdf5', 'w') as f:    \n",
    "    dset = f.create_dataset('data', data = np.arange(10))\n",
    "    #Read 3rd element\n",
    "    print('1. Third element is', dset[3])\n",
    "    #Read last element\n",
    "    print('2. Last element is', dset[-1])\n",
    "    #Read elements 2:7\n",
    "    print('3. Read elements 2 thru 7', dset[2:7])\n",
    "    #Read elements 4:8 step by 2\n",
    "    print('4. Read elements 4 thru 8, step by 2', dset[4:8:2])\n",
    "    #Read elements 4 to the end of the array\n",
    "    print('5. Read elements 4 to end of the array', dset[4:])\n",
    "    #Read elements 4 to second to last element, -1 is for the last element of the array\n",
    "    print('6. Read elements 4 to second to last element', dset[4:-1])   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lets look at multidimensional slicing and the ``...`` (ellipses) operator in HDF5. For demonstrating the concept we will be creating a 4D dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Shape of the slice is (10, 12)\n",
      "2a. Read 0th element of 1D dataset 11\n",
      "2b. Read using colon [11]\n",
      "2c. Read using ellipses [11]\n",
      "2c. Shape of the dataset is ()\n",
      "2d. Read using ellipses 10\n",
      "2e. Read using () 10\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('MultiDimSlicing.hdf5', 'w') as f:\n",
    "    arr = np.random.rand(20, 10, 12, 5)\n",
    "    dset1 = f.create_dataset('4d', data = arr, dtype = 'float32')\n",
    "    dset2 = f.create_dataset('1d', data = 11, shape = (1,))\n",
    "    dset3 = f.create_dataset('scalar', data = 10)\n",
    "    \n",
    "    # 1. Read the 4D data set's 0th element of in the first and fourth dimension. \n",
    "    #All other dimensions are completely read\n",
    "    \n",
    "    dset1_slice = dset1[0,...,0]\n",
    "    print('1. Shape of the slice is', dset1_slice.shape)\n",
    "    \n",
    "    print('2a. Read 0th element of 1D dataset', dset2[0])\n",
    "    print('2b. Read using colon', dset2[:])\n",
    "    print('2c. Read using ellipses', dset2[...])\n",
    "    \n",
    "    #print('2a. Read 0th element of scalar dataset', dset3[0])\n",
    "    #print('2b. Read using colon', dset3[:])\n",
    "    print('2c. Shape of the dataset is', dset3.shape)\n",
    "    print('2d. Read using ellipses', dset3[...])\n",
    "    print('2e. Read using ()', dset3[()])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When we slice as ``dset1[0,...,0]``, all elements of  $2^{nd}$ and $3^{rd}$ dimension are returned and thus the shape of the slice read is (10, 12).\n",
    "\n",
    "Reading the $0^{th}$ element of the 1D (of size 1 in this case) array with shape (1,) gives a scalar value, however, reading with the ellipses ``...`` or colon ``:`` returns this 1 element as an array. On other hand, reading the scalar value (data with shape ``()``) using index or colon operator will give an error (Commented out code), the same data can however be retrieved using ellipses operator and ``()`` operator\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "We will look at Boolean indexing. This is a common practice in Numpy and we did boolean indexing on this notebook earlier when demonstrating the performance of vectorized vs non vectorized code. The code ``val[val < 0] = 0`` is an example of boolean indexing where elements from the dataset are selected if the mask(``val < 0``) yields ``True``.\n",
    "\n",
    "Similar sort or boolean indexing can be done in case of HDF5 except that in HDF5 the index of the value in the mask is ``True`` is used to reference the elements from the dataset. As a consequence, instead of using boolean indexing on a dataset with lots of True values in mask, we often make changes to the Numpy array in memory and re set the value of the entire data set.\n",
    "\n",
    "Thus the following code snippets are identical\n",
    "```\n",
    "for ix in range(100):\n",
    "        val = dset2[ix, :]\n",
    "        val[val < 0] = 0\n",
    "        dset2[ix,:] = val\n",
    "```\n",
    "and \n",
    "\n",
    "```\n",
    "for ix in range(100):\n",
    "        val = dset2[ix, :]\n",
    "        dset2[ix,val < 0] = 0\n",
    "```\n",
    "The first variant is recommended (and should be benchmarked) in case the mask ``val < 0 `` gives a lots of ``True`` values\n",
    "\n",
    "---\n",
    "\n",
    "Another way to index elements in a dataset is using coordinate lists by passing a list of coordinates we would like to read as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Access using coordinate lists gives [0 2 4 6 8]\n",
      "2. Access using boolean indexing gives [0 2 4 6 8]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Indexing.hdf5', 'r') as f:\n",
    "    dset = f['data']\n",
    "    print('1. Access using coordinate lists gives', dset[[0, 2, 4, 6, 8]])\n",
    "    print('2. Access using boolean indexing gives', dset[np.arange(10) % 2 == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above accessing the data using coordinate index and boolean indexing gives the same data. Infact, boolean index internally is converted to a similar coordinate lists to retrieve data and thus is more efficient than boolean indexing as in above cases we have 50% ``True`` values in the mask (all even values in the between 0 and 9). There are few things to remember\n",
    "\n",
    "1. We can slice only one dimension using lists, thus a slicing like ``dset[[1, 2, 3], [4, 5, 6]]`` is not valid.\n",
    "2. All items in the list be specified in increasing order, which also means we cannot select the same index value twice.\n",
    "\n",
    "One interesting quirk is shown below. This is especially interesting as we use -1 to access the last element in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice is [2 3 4 9]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Indexing.hdf5', 'r') as f:\n",
    "    dset = f['data']\n",
    "    print('Slice is', dset[[-1, 2, 3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-1 accesses the last element but still needs to be placed ahead of the index 2, thus we **cannot** pass the list as ``[2, 3, 4, -1]`` which essentially is same as ``[2, 3, 4, 9]`` in above case as 9 is the last index of the list.\n",
    "\n",
    "The result returned however is not same as the order of the coordinate list. The result is same as when we would have passed ``[2, 3, 4, 9]``. This in my opinion is non intuitive and I personally would stay away from using negative index values (as of now while I am writing this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Slicing.hdf5', 'r') as f:\n",
    "    dset = f['arr1']\n",
    "    idx = np.arange(1000)\n",
    "    idx = idx < 4\n",
    "    #print(dset[[1, 2], [0, 1, 2, 3]])\n",
    "    #print(dset[[1, 2], idx])\n",
    "    print(dset[:, [1, 2]].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Above code snippet demonstrates that we can slice using boolean indexing or coordinate lists only in one dimension. Uncommenting either of the commented code above will give an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

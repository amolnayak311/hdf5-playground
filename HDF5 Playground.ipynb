{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook I will be reading the book ['Python and HDF5, Unlocking Scientific Data'](http://shop.oreilly.com/product/0636920030249.do) and try implementing the code given in the book, either as is or modified to suit my understanding or try some new thing out. The notebook will be filled with a lot of code accompanied by comments mentioning some important concept, intuition. To finish the book faster and get as much content as possible in the notebook I won't be giving the background for the HDF5 file format and other high level theoretical details which can be found in the book or other pages online but focus more on practical aspect of coding in Python and using HDF5 for storing scientific data.\n",
    "\n",
    "The HDF5 web site can be found [here](https://support.hdfgroup.org/HDF5/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the an example of data collected from weather station. Suppose we have 10 weather stations numbered from 1 to 10 for a date 1-Jan-2017 and each of them record temperature in Fahrenheit and wind speed in mph. We assume the numbers are integers and not floating point numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = 1\n",
    "month = 'Jan'\n",
    "year = 2017\n",
    "\n",
    "station_ids = range(1, 11, 1)\n",
    "np.random.seed(0)\n",
    "temperatures = np.asarray([np.random.randint(60, 80) for _ in station_ids])\n",
    "wind_speeds = np.asarray([np.random.randint(0, 10) for _ in station_ids])\n",
    "\n",
    "with h5py.File('weather.hdf5', 'w') as f:\n",
    "    for station_id, temperature, wind_speed in zip(station_ids, temperatures, wind_speeds):        \n",
    "        temperature_key = '/' + str(station_id) + '/temperature'\n",
    "        f[temperature_key] = temperature\n",
    "        f[temperature_key].attrs['date'] = 1\n",
    "        f[temperature_key].attrs['month'] = month\n",
    "        f[temperature_key].attrs['year'] = year\n",
    "        wind_speed_key = '/' + str(station_id) + '/wind_speed'\n",
    "        f[wind_speed_key] = wind_speed\n",
    "        f[wind_speed_key].attrs['date'] = 1\n",
    "        f[wind_speed_key].attrs['month'] = month\n",
    "        f[wind_speed_key].attrs['year'] = year\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now open this file and retrieve readings of the station 7 and confirm its what we wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature at station 7 is 69 ,wind speed recorded is 7 , the date these measurements were taken is 1-Jan-2017\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('weather.hdf5') as f:\n",
    "    station7_temperature = f['/7/temperature']\n",
    "    station7_wind_speed = f['/7/wind_speed']\n",
    "    assert station7_temperature.value == temperatures[6], 'Value not same as the one written'\n",
    "    assert station7_wind_speed.value == wind_speeds[6], 'Value not same as the one written'\n",
    "    temperature_node_attrs = dict([a for a in station7_temperature.attrs.items()])\n",
    "    print('Temperature at station 7 is', station7_temperature.value, ',wind speed recorded is'\n",
    "          ,station7_wind_speed.value, ', the date these measurements were taken is',\n",
    "         '%d-%s-%d'%(temperature_node_attrs['date'], temperature_node_attrs['month'], temperature_node_attrs['year']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 file is not entirely loaded in memory but only the data required and read is loaded. In above case the weathers file may have a lot of data but only the necessary information about station 7 was read in memory when requested\n",
    "\n",
    "Let's look at another example. where we create a dataset (we are yet to see what a dataset is).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the file BigArrayFile.hdf5 is 1400 bytes\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('BigArrayFile.hdf5', 'w') as f:\n",
    "    dataset = f.create_dataset('big', shape = (1024, 1024), dtype = 'float32')\n",
    "    \n",
    "stats = os.stat('BigArrayFile.hdf5')\n",
    "print('Size of the file BigArrayFile.hdf5 is',stats.st_size, 'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, we created an HDF5 file and created a data set called big in it of shape $1024 \\times 1024$ of type float32. Yet, the size of the file on the disk is 1400 bytes, let us set a byte at index (2, 2) with value 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the file BigArrayFile.hdf5 is 4195704 bytes\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('BigArrayFile.hdf5') as f:\n",
    "    dataset = f['big']\n",
    "    dataset[2, 2] = 2.0\n",
    "    \n",
    "stats = os.stat('BigArrayFile.hdf5')\n",
    "print('Size of the file BigArrayFile.hdf5 is',stats.st_size, 'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we see above, once we accessed the byte of the data set the entire dataset was flushed to the disk. The shape times 4 bytes(for float32) per location should take $1025 \\times 1024 \\times 8 = 4194304$, the size we see above is pretty close to this number as there HDF5 itself takes few bytes for the meta data. Also, an interesting point to note is that the dataset can be large in size (large enough to load all in memory), but only the bytes accessed will be loaded in memory.\n",
    " \n",
    " HDF5 also supports compression of the data. Lets create a dataset of same size $1024 \\times 1024$, but create the dataset using compression gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the file BigCompressedArrayFile.hdf5 is 4075 bytes\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('BigCompressedArrayFile.hdf5', 'w') as f:\n",
    "    dataset = f.create_dataset('big', shape = (1024, 1024), dtype = 'float32', compression = 'gzip')\n",
    "    dataset[2, 2] = 2.0\n",
    "    \n",
    "stats = os.stat('BigCompressedArrayFile.hdf5')\n",
    "print('Size of the file BigCompressedArrayFile.hdf5 is',stats.st_size, 'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, the file containing the dataset with same shape, but compressed has a much lower size of the data stored on the disk. There is however a tradeoff between space on disk and CPU time required to compress and decompress the contents. We will talk more on this later in the notebook.\n",
    "\n",
    "We have used ``h5py.File`` to open the file. The second parameter is the ``mode`` argument which can either be\n",
    "\n",
    "* r  : read only for existing file, fails when the provided file is not present\n",
    "* r+ : read/write for existing file, fails when the provided file is not present\n",
    "* w  : write, create a new file, truncates an existing file\n",
    "* w- : write, same as w except that it doesnt truncate an existing file but the operation fails\n",
    "* a  : read/write, if existing file not found, new one will be created (this if not the same in case of r+, but same as using either w or r+ depending on whether the file exists of not)\n",
    "\n",
    "---\n",
    "\n",
    "#### Drivers\n",
    "\n",
    "HDF5 drivers are the ones who map the bytes to be written to the low level disk bytes. Following are some important types of drivers\n",
    "\n",
    "* SEC2: Default driver that uses posix file system read/write functions to a single file\n",
    "* STDIO: Use the stdio.h for performing buffered read and write functions to single file\n",
    "* CORE: Performs read/write in memory for with option an option to create backing files on file system.\n",
    "* FAMILY: Partitions a large file in multiple chunks\n",
    "* MPIIO: Parallel HDF5 which allows multiple processes to read/write from same HDF5 file in parallel. We will see this type of driver in more details later.\n",
    "\n",
    "---\n",
    "\n",
    "### Datasets\n",
    "\n",
    "In this section we will look at datasets in HDF5\n",
    "\n",
    "Datasets are like Numpy array on disk. These datasets have a name, shape, type and can be sliced like an in memory Numpy array except that the entire dataset need not be loaded to memory and only the accessed slices from disk are loaded in memory as needed.\n",
    "\n",
    "Let us now create a Numpy array and create a data set out of it in an HDF5 file. Lets get some attributes of the dataset and the numpy array and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array type is int64 shape is (1, 5) type of arr is <class 'numpy.ndarray'>\n",
      "Dataset type is int64 shape is (1, 5) type of a is <class 'h5py._hl.dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3, 4, 5]])\n",
    "print('Numpy array type is', arr.dtype, 'shape is', arr.shape, 'type of arr is', type(arr))\n",
    "\n",
    "with h5py.File('DatasetTest.hdf5', 'w') as f:\n",
    "    f['arr'] = arr\n",
    "\n",
    "# Opening the file again, in read mode\n",
    "with h5py.File('DatasetTest.hdf5', 'r') as f:\n",
    "    a = f['arr']\n",
    "    print('Dataset type is', a.dtype, 'shape is', a.shape, 'type of a is', type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can see the similarity between Numpy and h5py API.\n",
    "\n",
    "Lets now read, modify the dataset and slice the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Arr type is int64 shape is (1, 5) type of a is <class 'numpy.ndarray'>\n",
      "2. Arr now is [[10 10 10 10 10]] Dataset is [[1 2 3 4 5]]\n",
      "3. Arr type is int64 shape is (2,) type is <class 'numpy.ndarray'> contents are [3 4]\n",
      "4. Dataset now is [[10 10 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('DatasetTest.hdf5', 'a') as f:\n",
    "    #1\n",
    "    arr = f['arr'][:]\n",
    "    print('1. Arr type is', arr.dtype, 'shape is', arr.shape, 'type of a is', type(arr))\n",
    "    \n",
    "    #2\n",
    "    arr[:] = 10\n",
    "    print('2. Arr now is', arr[:], 'Dataset is', f['arr'][:])\n",
    "    \n",
    "    #3\n",
    "    arr = f['arr'][0, 2:4]\n",
    "    print('3. Arr type is', arr.dtype, 'shape is', arr.shape, 'type is', type(arr), 'contents are', arr)\n",
    "    \n",
    "    #4\n",
    "    f['arr'][:] = 10\n",
    "    print('4. Dataset now is', f['arr'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have 4 outputs on the previous line and following are the observations\n",
    "\n",
    "1. We can read the entire dataset in memory using `:` on the dataset object of the HDF5 file and serialize the contents to a numpy array in memory. Be careful while performing this operation on a large dataset as the entire dataset is seldom needed top be loaded in memory or in the worst case the size may be much larger than the available memory. The `...` can be used instead of `:` to reference the entire dataset.\n",
    "2. Once in memory, this numpy array is just a copy and stale copy of the dataset in the file.  There is more synchronization between the array and the dataset in the file. As we see, any changes made to the contents of the array are local to the array and not reflected if we read the contents of the file back.\n",
    "3. The access here to the dataset is how we will be practically accessing the HDF5 dataset in the file. We slice a part of it as we need to read and only and only this slice is serialized to memory by the HDF5 library.\n",
    "4. Here we modify the entire dataset in the file. We would seldom do this in a real application especially for large datasets but only a slice of the dataset will be updated.\n",
    "\n",
    "---\n",
    "\n",
    "We will look at the how we can control the datatypes of the data that is read and written from and to the underlying HDF5 file.\n",
    "\n",
    "Suppose we have a numpy array with double precision datatype float64 but we are ok to store the data as float32 to save some extra disk space effectively making the IO fast.\n",
    "\n",
    "Let us first write the same array of double precision to the underlying file, one using double precision and one using single precision floating point type and compare the size of the files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of file holding float64 array is 82144 , Size of file holding float32 is 42144\n"
     ]
    }
   ],
   "source": [
    "arr = np.random.rand(100, 100)\n",
    "\n",
    "with h5py.File('DoublePrecision.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset('arr', data = arr, dtype = 'float64')\n",
    "    \n",
    "with h5py.File('SinglePrecision.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset('arr', data = arr, dtype = 'float32')\n",
    "    \n",
    "dblstat = os.stat('DoublePrecision.hdf5')\n",
    "singlestat = os.stat('SinglePrecision.hdf5')\n",
    "\n",
    "print('Size of file holding float64 array is', dblstat.st_size, \n",
    "      ', Size of file holding float32 is', singlestat.st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above, the size of the file is halved by changing the datatype. This is a good optimization provided that the change of double to single precision float doesn't impact the application using the data. Similar choices can be made when using integer datatype in choosing between 8, 16 and 32 bits depending on the expected range of the values the data can take. This is no different than choosing datatypes for a variable in programming languages\n",
    "\n",
    "Let's say we have written the float64 numpy array as a float32 array. This means when we read the value back, the datatype of the numpy array will be float32 which might not be desirable and we would want to seamlessly read the value into a float64 numpy array. There are a couple of ways we can do that as we see below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of arr is float32 dtype of arr is float64 dtype of arr2 is float64\n",
      "Equality check of arr and arr1 gives True\n",
      "Equality check of arr1 and arr2 gives True\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('SinglePrecision.hdf5', 'r') as f:\n",
    "    #1\n",
    "    ds = f['arr']\n",
    "    arr = ds[:]\n",
    "    #2\n",
    "    arr1 = np.empty(shape = ds.shape, dtype = 'float64')\n",
    "    ds.read_direct(arr1)\n",
    "    #3\n",
    "    with ds.astype('float64'):\n",
    "        arr2 = ds[:]\n",
    "\n",
    "print('dtype of arr is', arr.dtype, 'dtype of arr is', arr1.dtype, 'dtype of arr2 is', arr2.dtype)\n",
    "print('Equality check of arr and arr1 gives',(arr == arr1).all())\n",
    "print('Equality check of arr1 and arr2 gives',(arr2 == arr1).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see from the above output, by default, the array is read as a float32 and the datatype of the original array written is lost. If we want to read the object back the object as a float64 array, the first approach (given in #2 above) is to initialize an empty numpy array of the desired target type and pass it to ``read_direct`` method of the HDF5 dataset instance. Another alternate(and cleaner in my opinion) is given in #3\n",
    "\n",
    "---\n",
    "\n",
    "Let us see the default values of an empty dataset and see how to use a different default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Using Defaults: arri,  [[0 0]] arrf,  [[ 0.  0.]]\n",
      "2. Using FillValues: arri,  [[-1 -1]] arrf,  [[ nan  nan]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('FillValue.hdf5', 'w') as f:\n",
    "    dseti = f.create_dataset('empty_ints', shape = (1, 2), dtype = 'int16')\n",
    "    dsetf = f.create_dataset('empty_floats', shape = (1, 2), dtype = 'float32')\n",
    "    arri = dseti[:]\n",
    "    arrf = dsetf[:]\n",
    "    print('1. Using Defaults: arri, ', arri, 'arrf, ', arrf)\n",
    "    dseti = f.create_dataset('fillv_ints', shape = (1, 2), dtype = 'int16', fillvalue = -1)\n",
    "    dsetf = f.create_dataset('fillv_floats', shape = (1, 2), dtype = 'float32', fillvalue = float('nan'))\n",
    "    arri = dseti[:]\n",
    "    arrf = dsetf[:]\n",
    "    print('2. Using FillValues: arri, ', arri, 'arrf, ', arrf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above, the default value for int as well as float is 0. The domain of the application might have 0 as a valid value and thus we desire to different value which clearly identifies the value at that location in the array being absent and not a valid value. In such case we can use the ``fillvalue`` parameter to use a different fill value other than 0.\n",
    "\n",
    "Suppose our application domain requires a value to be a non negative value for int fields and a non Nan value to be used for floating point numbers, it makes sense to give a negative value as a fill value for integers and a NaN for floats as a default value.\n",
    "\n",
    "---\n",
    "\n",
    "### Reading and writing Data\n",
    "\n",
    "Let us create a dataset of size $100 \\times 1000 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"arr1\": shape (100, 1000), type \"<f8\">\n",
      "Slice shape (10, 50) , Slice type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "arr = np.random.randn(100, 1000)\n",
    "with h5py.File('Slicing.hdf5', 'w') as f:\n",
    "    f['arr1'] = arr\n",
    "    f['arr2'] = arr\n",
    "    dset = f['arr1']\n",
    "    #1\n",
    "    print(dset)\n",
    "    slice = dset[10:20, 20:70]\n",
    "#2\n",
    "print('Slice shape', slice.shape, ', Slice type', type(slice))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above, we have two datasets of size $100 \\times 1000$\n",
    "If we want a slice of it of ``dset[10:20, 20:70]`` we get a ``numpy.ndarray`` of shape `(10, 50)` and following is what happenes when we request the slice\n",
    "\n",
    "1. The shape of the data is figured out by h5py, in this case ``(10, 50)``\n",
    "2. An empty numpy array is instantiated of this size\n",
    "3. Appropriate portions of the data set are selected, this may include the necessary IO operations to take place on the underlying HDF5 file.\n",
    "4. Copy the data read to the empty numpy array instantiated\n",
    "5. Return the numpy array\n",
    "\n",
    "All these operations are performed before accessing any portion of the dataset on the HDF5 file. It is thus highly recommended to vectorize the operations as far as possible. Following example demonstrates vectorzation.\n",
    "\n",
    "Suppose we want to chop the values of the dataset with values less than 0 to 0. We have the following two ways\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 2.86 s, total: 33.9 s\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Non Vectorized\n",
    "with h5py.File('Slicing.hdf5', 'a') as f:\n",
    "    dset1 = f['arr1']\n",
    "    \n",
    "    for ix in range(100):\n",
    "        for iy in range(1000):\n",
    "            val = dset1[ix, iy]\n",
    "            if val < 0: dset1[ix, iy] = 0\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61.2 ms, sys: 6.92 ms, total: 68.1 ms\n",
      "Wall time: 67.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Vectorized\n",
    "with h5py.File('Slicing.hdf5', 'a') as f:\n",
    "    dset2 = f['arr2']    \n",
    "    \n",
    "    for ix in range(100):\n",
    "        val = dset2[ix, :]\n",
    "        val[val < 0] = 0\n",
    "        dset2[ix,:] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above there is a signficant performance difference in updating the matrix element by element in a HDF5 dataset vs vectoring the operations along the longer dimension. Note that there is nothing stopping us from loading the entire data set in memory for the matrix of $100 \\times 1000$ case above, but for real world datasets we won't be loading the entire dataset in memory for manipulation, instead we will be reading the large chunk of data in memory and vectorise the operation on along these slices in a loop. It is interesting to see how the operation using slicing effectively scales up with such a small dataset. You can imagine the performance benefits on the real world, large dataset containing millions of elements\n",
    "\n",
    "Finally we want to ensure that two matrices contain identical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two data sets equality check gives True\n",
      "Negative number check gives True\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Slicing.hdf5', 'r') as f:\n",
    "    dset1 = f['arr1'][:]\n",
    "    dset2 = f['arr2'][:]\n",
    "    print('Two data sets equality check gives', (dset1 == dset2).all())\n",
    "    print('Negative number check gives', (dset1 >= 0).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You might have noticed that we use indexing of datasets in HDF5 is similar to Numpy datasets. Following snippet gives some ways to access the dataset elements. \n",
    "\n",
    "An important thing to remember is we **cannot** read the dataset in reverse order line a Numpy array\n",
    "by giving the range as ``-1:0`` also the step value has to be a non negative number where the format of the array index is ``[start]:[end]:[step]``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Third element is 3\n",
      "2. Last element is 9\n",
      "3. Read elements 2 thru 7 [2 3 4 5 6]\n",
      "4. Read elements 4 thru 8, step by 2 [4 6]\n",
      "5. Read elements 4 to end of the array [4 5 6 7 8 9]\n",
      "6. Read elements 4 to second to last element [4 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Indexing.hdf5', 'w') as f:    \n",
    "    dset = f.create_dataset('data', data = np.arange(10))\n",
    "    #Read 3rd element\n",
    "    print('1. Third element is', dset[3])\n",
    "    #Read last element\n",
    "    print('2. Last element is', dset[-1])\n",
    "    #Read elements 2:7\n",
    "    print('3. Read elements 2 thru 7', dset[2:7])\n",
    "    #Read elements 4:8 step by 2\n",
    "    print('4. Read elements 4 thru 8, step by 2', dset[4:8:2])\n",
    "    #Read elements 4 to the end of the array\n",
    "    print('5. Read elements 4 to end of the array', dset[4:])\n",
    "    #Read elements 4 to second to last element, -1 is for the last element of the array\n",
    "    print('6. Read elements 4 to second to last element', dset[4:-1])   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lets look at multidimensional slicing and the ``...`` (ellipses) operator in HDF5. For demonstrating the concept we will be creating a 4D dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Shape of the slice is (10, 12)\n",
      "2a. Read 0th element of 1D dataset 11\n",
      "2b. Read using colon [11]\n",
      "2c. Read using ellipses [11]\n",
      "2c. Shape of the dataset is ()\n",
      "2d. Read using ellipses 10\n",
      "2e. Read using () 10\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('MultiDimSlicing.hdf5', 'w') as f:\n",
    "    arr = np.random.rand(20, 10, 12, 5)\n",
    "    dset1 = f.create_dataset('4d', data = arr, dtype = 'float32')\n",
    "    dset2 = f.create_dataset('1d', data = 11, shape = (1,))\n",
    "    dset3 = f.create_dataset('scalar', data = 10)\n",
    "    \n",
    "    # 1. Read the 4D data set's 0th element of in the first and fourth dimension. \n",
    "    #All other dimensions are completely read\n",
    "    \n",
    "    dset1_slice = dset1[0,...,0]\n",
    "    print('1. Shape of the slice is', dset1_slice.shape)\n",
    "    \n",
    "    print('2a. Read 0th element of 1D dataset', dset2[0])\n",
    "    print('2b. Read using colon', dset2[:])\n",
    "    print('2c. Read using ellipses', dset2[...])\n",
    "    \n",
    "    #print('2a. Read 0th element of scalar dataset', dset3[0])\n",
    "    #print('2b. Read using colon', dset3[:])\n",
    "    print('2c. Shape of the dataset is', dset3.shape)\n",
    "    print('2d. Read using ellipses', dset3[...])\n",
    "    print('2e. Read using ()', dset3[()])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When we slice as ``dset1[0,...,0]``, all elements of  $2^{nd}$ and $3^{rd}$ dimension are returned and thus the shape of the slice read is (10, 12).\n",
    "\n",
    "Reading the $0^{th}$ element of the 1D (of size 1 in this case) array with shape (1,) gives a scalar value, however, reading with the ellipses ``...`` or colon ``:`` returns this 1 element as an array. On other hand, reading the scalar value (data with shape ``()``) using index or colon operator will give an error (Commented out code), the same data can however be retrieved using ellipses operator and ``()`` operator\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "We will look at Boolean indexing. This is a common practice in Numpy and we did boolean indexing on this notebook earlier when demonstrating the performance of vectorized vs non vectorized code. The code ``val[val < 0] = 0`` is an example of boolean indexing where elements from the dataset are selected if the mask(``val < 0``) yields ``True``.\n",
    "\n",
    "Similar sort or boolean indexing can be done in case of HDF5 except that in HDF5 the index of the value in the mask is ``True`` is used to reference the elements from the dataset. As a consequence, instead of using boolean indexing on a dataset with lots of True values in mask, we often make changes to the Numpy array in memory and re set the value of the entire data set.\n",
    "\n",
    "Thus the following code snippets are identical\n",
    "```\n",
    "for ix in range(100):\n",
    "        val = dset2[ix, :]\n",
    "        val[val < 0] = 0\n",
    "        dset2[ix,:] = val\n",
    "```\n",
    "and \n",
    "\n",
    "```\n",
    "for ix in range(100):\n",
    "        val = dset2[ix, :]\n",
    "        dset2[ix,val < 0] = 0\n",
    "```\n",
    "The first variant is recommended (and should be benchmarked) in case the mask ``val < 0 `` gives a lots of ``True`` values\n",
    "\n",
    "---\n",
    "\n",
    "Another way to index elements in a dataset is using coordinate lists by passing a list of coordinates we would like to read as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Access using coordinate lists gives [0 2 4 6 8]\n",
      "2. Access using boolean indexing gives [0 2 4 6 8]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Indexing.hdf5', 'r') as f:\n",
    "    dset = f['data']\n",
    "    print('1. Access using coordinate lists gives', dset[[0, 2, 4, 6, 8]])\n",
    "    print('2. Access using boolean indexing gives', dset[np.arange(10) % 2 == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above accessing the data using coordinate index and boolean indexing gives the same data. Infact, boolean index internally is converted to a similar coordinate lists to retrieve data and thus is more efficient than boolean indexing as in above cases we have 50% ``True`` values in the mask (all even values in the between 0 and 9). There are few things to remember\n",
    "\n",
    "1. We can slice only one dimension using lists, thus a slicing like ``dset[[1, 2, 3], [4, 5, 6]]`` is not valid.\n",
    "2. All items in the list be specified in increasing order, which also means we cannot select the same index value twice.\n",
    "\n",
    "One interesting quirk is shown below. This is especially interesting as we use -1 to access the last element in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice is [2 3 4 9]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Indexing.hdf5', 'r') as f:\n",
    "    dset = f['data']\n",
    "    print('Slice is', dset[[-1, 2, 3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-1 accesses the last element but still needs to be placed ahead of the index 2, thus we **cannot** pass the list as ``[2, 3, 4, -1]`` which essentially is same as ``[2, 3, 4, 9]`` in above case as 9 is the last index of the list.\n",
    "\n",
    "The result returned however is not same as the order of the coordinate list. The result is same as when we would have passed ``[2, 3, 4, 9]``. This in my opinion is non intuitive and I personally would stay away from using negative index values (as of now while I am writing this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Slicing.hdf5', 'r') as f:\n",
    "    dset = f['arr1']\n",
    "    idx = np.arange(1000)\n",
    "    idx = idx < 4\n",
    "    #print(dset[[1, 2], [0, 1, 2, 3]])\n",
    "    #print(dset[[1, 2], idx])\n",
    "    print(dset[:, [1, 2]].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Above code snippet demonstrates that we can slice using boolean indexing or coordinate lists only in one dimension. Uncommenting either of the commented code above will give an error\n",
    "\n",
    "---\n",
    "\n",
    "We will first look at an concept from Numpy which lets us create a ``slice`` for taking slices of any array.\n",
    "The class ``slice`` in Numpy is instantiated whenever we access any numpy array using a start stop and step values.\n",
    "For e.g ``arr[1:6:2]`` takes a slice of a numpy array ``arr``. The value ``1:6:2`` is converted to a slice object which is then used to take the slice of the array ``arr``. Let's look at it in action in the following example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Slice using 1:6:2 gives [1 3 5]\n",
      "2. Slicing using slice object gives [1 3 5]\n",
      "3. Slice object is slice(1, 6, 2)\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(10)\n",
    "print('1. Slice using 1:6:2 gives', arr[1:6:2])\n",
    "s = np.s_[1:6:2]\n",
    "print('2. Slicing using slice object gives', arr[s])\n",
    "print('3. Slice object is', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see in the above code snippet, both invocations give the same result, where by internally the value ``1:6:2`` in the former way, which also is the more intuitive and common way of indexing an array, is compiled to a slice object before getting the actual slice. A slice is created in Numpy using ``np.s_`` which creates a slice object. The slice has the signature ``slice(start, stop[, step])``\n",
    "\n",
    "This slice object can be used multiple times to slice multiple arrays. Whenever we have an array with multiple dimensions the return value of ``np.s_`` is a tuple giving a slice object or a scalar of tuples, created one for each dimension. Following couple of examples demonstrate this concept. We instantiate slices for 2 and three dimension arrays and we see the number of elements in the tuple are same as the dimension of the array we intend to slice. Whenever a scalar is used to access the element in a dimension as in case of ``s2``, a scalar is returned in the tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 is (slice(0, 5, None), slice(2, 10, 2)) s2 is  (0, slice(1, 5, None), slice(2, 8, None))\n"
     ]
    }
   ],
   "source": [
    "s1 = np.s_[0:5, 2:10:2]\n",
    "s2 = np.s_[0, 1:5, 2:8]\n",
    "print('s1 is', s1, 's2 is ', s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now that we have introduced what ``np.s_`` does, lets revisit ``read_direct`` we briefly introduced earlier in the notebook. the ``read_direct`` method gets close to the native C implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Small slice of out [[ 0.          0.        ]\n",
      " [ 0.40741902  0.86558573]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Slicing.hdf5', 'r') as f:\n",
    "    dset = f['arr1']\n",
    "    out = np.empty(dset.shape, dtype = np.float64)\n",
    "    dset.read_direct(out)\n",
    "    print('1. Small slice of out', out[0:2, 0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above implementation reads the entire dataset in the numpy array which seldom is done on real world datasets. Instead we want to read a portion of it. Following example demonstrates how to read the entire first two rows in an empty numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Initialize the out array of desired shape\n",
    "\n",
    "out1 = np.empty(shape = (2, 1000), dtype = np.float64)\n",
    "#2. Instantiate the slice instance(s) for each dimension giving the target indices of out1, in our case the target \n",
    "# array is of exact size as the value we plan to read, hence [:,:]\n",
    "out_slice = np.s_[:,:]\n",
    "\n",
    "#3. The slice instances for the input dataset. Since we want to read the first two rows and all columns of the \n",
    "# dataset we instantiate it as follows. Note that the shape of the slices generated by in_slice and out_slice should\n",
    "# be same (if not same the value should be broadcastable, but we will skip that possibility and keep it simple)\n",
    "in_slice = np.s_[[0,1], :]\n",
    "\n",
    "with h5py.File('Slicing.hdf5', 'r') as f:\n",
    "    dset = f['arr1']\n",
    "    dset.read_direct(out1, source_sel = in_slice, dest_sel = out_slice)\n",
    "    \n",
    "assert (out1 == out[0:2, :]).all(), 'out1 == out[0:2, :] expected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "--- \n",
    "\n",
    "Lets do a small test of endians in Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5113998769666068\n",
      "1.2289076369488612\n"
     ]
    }
   ],
   "source": [
    "le = np.ones((1000, 1000), dtype = '<f4')\n",
    "be = np.ones((1000, 1000), dtype = '>f4')\n",
    "print(timeit.timeit(le.mean, number = 1000))\n",
    "print(timeit.timeit(be.mean, number = 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Natively, Intel processors use Little Endian format to store multi byte numbers. The above code is exactly identical and generates same results, excapt that one of them is more that 2 as faster than other.\n",
    "The array that uses little endian datatype is faster than the one using big endian. In HDF5 file both endian formats are supported, however, when loaded they are are loaded in the same format as the ones they are stored in. If they are stored in Big endian format, when loaded in memory of a computer using a Little endian format, we have to pay the price for the performance. This is just one of the performance problems we may face if the datatypes aren't considered carefully\n",
    "\n",
    "---\n",
    "\n",
    "Finally, lets look at resizing datasets.\n",
    "\n",
    "Any dataset that we create without the maxshape attribute not only has a ``maxshape`` which defaults to ``shape`` but is continuous, thus any attempt to resize it will throw an exception saying *Only chunked datasets can be resized*. \n",
    "\n",
    "Specifying the ``maxshape`` attribute will make the dataset chunked and will allow extending the dataset to the value of the maxshape in each of the dimensions.\n",
    "\n",
    "If we want unlimited extension, specify the value in a given dimension as None. For e.g., suppose we just want to append rows to the (2, 2) dataset, the ``maxshape`` will be (None, 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1a. Shape of twobytwo is (2, 2)\n",
      "1b. Maxshape if twobytwo is (2, 2)\n",
      "2a. Shape of twobytwochunked is (2, 2)\n",
      "2b. Maxshape if twobytwochunked is (2, 2)\n",
      "2c. Resizing twobytwochunked to (1, 2)\n",
      "2d. Shape of twobytwochunked is (1, 2)\n",
      "3a. Shape of nbytwochunked is (2, 2)\n",
      "3b. Maxshape if nbytwochunked is (None, 2)\n",
      "3c. Resizing nbytwochunked to (10, 2)\n",
      "3d. Shape of nbytwochunked is (10, 2)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Resize.hdf5', 'w') as f:\n",
    "    twobytwo = f.create_dataset('twobytwo', shape = (2, 2))\n",
    "    print('1a. Shape of twobytwo is', twobytwo.shape)\n",
    "    print('1b. Maxshape if twobytwo is', twobytwo.maxshape)\n",
    "    #Uncommenting the following will throw an exception    \n",
    "    #twobytwo.resize(1, 1)\n",
    "    twobytwochunked = f.create_dataset('twobytwochunked', shape = (2, 2), maxshape = (2, 2))\n",
    "    print('2a. Shape of twobytwochunked is', twobytwochunked.shape)\n",
    "    print('2b. Maxshape if twobytwochunked is', twobytwochunked.maxshape)\n",
    "    print('2c. Resizing twobytwochunked to (1, 2)')\n",
    "    twobytwochunked.resize((1, 2))\n",
    "    print('2d. Shape of twobytwochunked is', twobytwochunked.shape)\n",
    "    #Uncommenting following will give an error as maxshape is (2, 2)\n",
    "    #twobytwochunked.resize((3, 2))\n",
    "    nbytwochunked = f.create_dataset('nbytwochunked', shape = (2, 2), maxshape = (None, 2))\n",
    "    print('3a. Shape of nbytwochunked is', nbytwochunked.shape)\n",
    "    print('3b. Maxshape if nbytwochunked is', nbytwochunked.maxshape)\n",
    "    print('3c. Resizing nbytwochunked to (10, 2)')\n",
    "    nbytwochunked.resize((10, 2))\n",
    "    print('3d. Shape of nbytwochunked is', nbytwochunked.shape)\n",
    "    \n",
    "    #Uncommenting the following will fail as the max shape for the second dimension is 2\n",
    "    #nbytwochunked.resize((10, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Resizing dataset in HDF5 is not same as resizing a Numpy array. Lets look at resizing Numpy array of size 2, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Shape is array is (2, 2)\n",
      "2. Contents of nparr are [[1, 2], [3, 4]]\n",
      "3. Shape is array is (1, 4)\n",
      "4. Contents of nparr are [[1, 2, 3, 4]]\n",
      "5. Shape is array is (1, 10)\n",
      "6. Contents of nparr are [[1, 2, 3, 4, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "nparr = np.array([[1, 2], [3, 4]])\n",
    "print('1. Shape is array is', nparr.shape)\n",
    "print('2. Contents of nparr are', nparr.tolist())\n",
    "nparr.resize((1, 4))\n",
    "print('3. Shape is array is', nparr.shape)\n",
    "print('4. Contents of nparr are', nparr.tolist())\n",
    "nparr.resize((1, 10))\n",
    "print('5. Shape is array is', nparr.shape)\n",
    "print('6. Contents of nparr are', nparr.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above, when we resize, existing elements of the Numpy array are retained, they just are reordered. If the resizing exceeds the number of elements in the array, the remaining elements are defaulted to 0.\n",
    "\n",
    "Lets see what happens when we reshape the HDF5 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Shape of origtwobytwo is (2, 2)\n",
      "2. Data of origtwobytwo is [[1, 2], [3, 4]]\n",
      "3. Resizing to shape (1, 4)\n",
      "4. Shape of origtwobytwo is (1, 4)\n",
      "5. Data of origtwobytwo is [[1, 2, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Resize2.hdf5', 'w') as f:\n",
    "    origtwobytwo = f.create_dataset('origtwobytwo', data = np.array([[1, 2], [3, 4]]), maxshape = (None, None))\n",
    "    print('1. Shape of origtwobytwo is', origtwobytwo.shape)\n",
    "    print('2. Data of origtwobytwo is', origtwobytwo[...].tolist())\n",
    "    print('3. Resizing to shape (1, 4)')\n",
    "    origtwobytwo.resize((1, 4))\n",
    "    print('4. Shape of origtwobytwo is', origtwobytwo.shape)\n",
    "    print('5. Data of origtwobytwo is', origtwobytwo[...].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After resize, unlike Numpy array where the data is shuffled, in HDF5 dataset, the data is lost. Since we kept only one row in the resized dataset, the first row was retained and everything was made 0 thus losing data. This is an important fact to keep in mind when we we reduce the size of any dimension of an HDF5 data with existing data.\n",
    "The values filled in this case are 0 and can be changed to any other value using the ``fillvalue`` parameter when we create the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Chunking and Compression\n",
    "\n",
    "Storing multidimensional data on disk is flattening the data and storing it as a stream of byte array. Suppose we have a dataset of shape (100, 480, 640) which holds 100 images of size $640 \\times 480$. The fast moving dimension is the last dimension (of size 640) and is the one whose values are stored continuously. Thus one image is stored as 640 pixel values stored continuously 480 times which is $640 \\times 480 = 307200$ bytes (assuming one byte is used to store a pixel value and the image is a gray scale as we dont have 3 channels for each pixel). \n",
    "This is just one image, such 307200 bytes will be stored 100 times for the entire dataset.\n",
    "\n",
    "Considering we have a spinning disk drive, there is a cost in moving the head to the desired location and read a stream of bytes. Thus storing the bytes used together continuously is most efficient. \n",
    "If we want the slice ``[0, :, :]``, its effifient as all these bytes are stored continuously on the underlying storage. However, consider we are trying to access the slice ``[0, 0:64, 0:64]``. This slice will access the first 64 bytes of first 64 rows of 640 pixels. This is as good as pulling a corner $64 \\times 64$ pixel block of an image. This access pattern is not the most efficient way.\n",
    "\n",
    "What if we already knew this access pattern, is there a way we can store this data in chunks of a predetermined size and then later retrieve them efficiently?\n",
    "\n",
    "While creating a dataset using ``create_dataset`` we can specify the chunk size. For above case where we intend to access the image chunks in $64 \\times 64$ pixels, an appropriate chunk size would be (1, 64, 64). \n",
    "Internally, the image bytes will be split in chunks of $64 \\times 64$ and stored in the file. A BTree index will be maintained by HDF5 which will allow us to locate the location of the chunk stored on the disk and retrieve the continuous bytes stored for the chunk.\n",
    "\n",
    "Since chunks are a fixed set of bytes stored on the disk, we can have filters configured which intercept the read and write calls allowing us to compress and decompress the data before writing and reading from the application, transparent to the application.\n",
    "\n",
    "Let's write a small code snippet which shows how to create chunked datasets we mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Shape of the dataset is (100, 480, 640)\n",
      "2. Chunk size of the dataset is (1, 64, 64)\n",
      "3. Maxshape of the dataset is (100, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('ChunkedSimple.hdf5', 'w') as f:\n",
    "    ds = f.create_dataset('simplechunked', shape = (100, 480, 640), dtype = 'int8', chunks = (1, 64, 64))\n",
    "    print('1. Shape of the dataset is', ds.shape)\n",
    "    print('2. Chunk size of the dataset is', ds.chunks)\n",
    "    print('3. Maxshape of the dataset is', ds.maxshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Chunks are enabled whenever we make the dataset resizable or compression is enabled. In this case, the chunk size if automatically chosen by the library. Auto chunking is useful only when the sole objective is to make the dataset resizable and compression enabled and not to do with the performance of slicing the dataset.\n",
    "If slicing performance and due to disk access pattern and the way data is stored is important, specifying the necessary chunking based on the data access pattern is important.\n",
    "\n",
    "When manually picking the chunk size, considering the following is important.\n",
    "\n",
    "1. Larger chunks result is less total number of chunks in the dataset thus reducing the size of the chunk Btree. Smaller the tree, faster would be the lookup.\n",
    "2. Chunk loading is either complete or nothing. So even if we wish to read a single location from the dataset, entire chunk will be loaded.\n",
    "3. Chunk cache will not cache chunks larger than 1 MB. Even for chunks smaller than 1 MB, the number of chunks in the cache size if limited.\n",
    "\n",
    "---\n",
    "\n",
    "#### Filters\n",
    "\n",
    "HDF5 has a concept of Filter Pipeline where the data written or read is passed through a sequence of filters which can perform any operation like compression, add metadata, calculate checksum etc. While reading the filters are invoked with the data in the reverse order to that of the order during write and the operation performed will be reverse of what was done, that is for compression/decompression filter, if the data was compressed during write, during read it will perfom decompression. Also, once a data set of created with a list of filters, they cannot be modified.\n",
    "\n",
    "---\n",
    "\n",
    "Let's first look at compression filters. The most common type of filter in use is GZIP Filter, also called ***DEFLATE*** filter. Setting up a dataset for compression is as simple as follows. Note that the compression enables chunking too.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Compression is  gzip\n",
      "2. Chunk size is (50, 50)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('GZIPCompressed.hdf5', 'w') as f:\n",
    "    ds = f.create_dataset('compressed', shape = (100, 100), dtype = 'float32', compression = 'gzip')\n",
    "    print('1. Compression is ', ds.compression)\n",
    "    print('2. Chunk size is', ds.chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Above code snippet shows how we can simply compress the dataset. The way we read/write to dataset is independent of whether compression filter is enabled or not.\n",
    "\n",
    "GZIP/DEFLATER filter has following features.\n",
    "\n",
    "1. Its shipped with HDF5 Library\n",
    "2. Supports all HDF5 datatypes\n",
    "3. It gives moderate to slow speed compression\n",
    "\n",
    "Another possible choice is LZF Compression which has the following advantages\n",
    "\n",
    "1. Works with all HDF5 types\n",
    "2. Fast compression and decompression\n",
    "\n",
    "Some things to keep in mind before using LZF compression is \n",
    "\n",
    "1. It works only on Python Platforms as it ships with h5py.\n",
    "2. Compression ratio is lower than GZIP, and thus the files would be bigger in size than GZIP compressed files.\n",
    "\n",
    "\n",
    "There are a lot of [Third Party Filters](https://support.hdfgroup.org/services/contributions.html#filters) available and with different compression/decompression filters available. If you intend to distribute the HDF5 files to other consumers, you need to ensure that the clients are configured and are able to use those filters while reading/writing to the distributed files.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Another type of filter thats used in conjunction is ***SHUFFLE*** filter. Suppose we have a 32 bit integer datatype and the numbers stored in them are affecting mostly the lower two bytes, then the first two bytes are mostly all zeros. Compression algorithms give better compression when we have identical bytes following each other. Shuffle filter thus packs all bytes in a way such that all first bytes of the 4 byte number are together followed by the second, third and fourth. This way, the entropy is not scattered all over the place but concentrated around where the least significant bytes are packed allowing better compression.\n",
    "Following are the points to remember when using SHUFFLE filter\n",
    "\n",
    "1. Its relatively inexpensive to use this filter\n",
    "2. Meaningful only if compression is enabled\n",
    "3. Available in all HDF5 distributions.\n",
    "\n",
    "Enabling shuffle filter is as east as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Compression is gzip\n",
      "2. Chunking size is (50, 50)\n",
      "3. Shuffle enabled is True\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('CompressAndShuffle.hdf5', 'w') as f:\n",
    "    ds = f.create_dataset('compress_and_shuffle', shape = (100, 100), \n",
    "                          dtype = 'int32', compression = 'gzip', shuffle = True)\n",
    "    print('1. Compression is', ds.compression)\n",
    "    print('2. Chunking size is', ds.chunks)\n",
    "    print('3. Shuffle enabled is', ds.shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will look at another filter which computes the checksums to validate data integrity whenever we read a dataset.\n",
    "In case of corruption of HDF5 data file, we may not at times read the bytes those were written to the chunk. In such cases its desired to store the checksum of the chunk in the chunk's metadata. Whenever we read a chunk back, the checksum is computed and compared with the checksum stored in the metadata when it was written. \n",
    "HDF5 ships with a default filter for this purpose called the ***FLETCHER32*** filter as it uses 32 bit [Fletcher's Checksum](https://en.wikipedia.org/wiki/Fletcher%27s_checksum). \n",
    "\n",
    "Enabling checksum is as simple as the following code snippet and it has the folliwng features\n",
    "\n",
    "1. Inbuilt and comes shipped with HDF5 stack\n",
    "2. Very fast \n",
    "3. Compatible with other lossless filters configured.\n",
    "4. When using ``fletcher32`` checksum filter compression is automatically enabled and auto chunking chooses the chunk sizes for the dataset unless we specify one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Chunk size is (100, 100)\n",
      "2. Fletcher32 enabled?, True\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Checksum.hdf5', 'w') as f:\n",
    "    ds = f.create_dataset('checksum', shape = (100, 100), dtype = 'int8', fletcher32 = True)\n",
    "    print('1. Chunk size is', ds.chunks)\n",
    "    print('2. Fletcher32 enabled?', ds.fletcher32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Groups, Links and Iteration\n",
    "\n",
    "So far all the datasets we create are created in the HDF5 file directly. Which is same as putting all files on the root partition of the file system. For managing our files we create the folders(and subfolders) in the root directory and place our files in it. HDF5 files have something similar where we can create a *hierarchy* of groups in the file. When ever we create a file as ``h5py.File('...', 'w')`` we are creating a group, the root. We are then free to create multiple nested groups in this file. Lets look at how we can create these groups in an HDF5 file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. g1 is <HDF5 group \"/group1\" (2 members)>\n",
      "2. g2 is <HDF5 group \"/group1/group2\" (0 members)>\n",
      "3. g3 is <HDF5 group \"/group1/group3\" (0 members)>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Groups.hdf5', 'w') as f:\n",
    "    g1 = f.create_group('group1')\n",
    "    g2 = g1.create_group('group2')\n",
    "    #Alternativly and more readable, we create the group as follows and all intermediate\n",
    "    #missing groups are automatically created. Kind of like mkdir -p ... in Unix\n",
    "    g3 = f.create_group('/group1/group3')\n",
    "    print('1. g1 is', g1)\n",
    "    print('2. g2 is', g2)\n",
    "    print('3. g3 is', g3)\n",
    "    \n",
    "    #Lets create some datasets in this \n",
    "    f['dset1'] = np.random.rand(10, 10)\n",
    "    g3['dset2'] = np.random.rand(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lets look at how we can read the datasets we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('Groups.hdf5', 'r') as f:    \n",
    "    dset1 = f['dset1']\n",
    "    \n",
    "    #Two ways of accessing dset2\n",
    "    dset2_1 = f['group1']['group3']['dset2'][:]  #1. Works, but not performant\n",
    "    dset2_2 = f['group1/group3/dset2'][:]        #2. Same as 1, but more performant\n",
    "    assert (dset2_1 == dset2_2).all(), 'Expected both datasets to be identical'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above there are a couple of ways we can access the dataset in a group. The most recommended and efficient approach is to access the dataset by passing in the entire path separated by '/' rather that the approach given in 1. \n",
    "\n",
    "Accessing a dataset which doesn't exist using ``group['dset_name']`` will throw an exception. If however we dont want recover by catching an exception but checking the return value, the ``get`` method should be used on the group as we see in the following code snippet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caught KeyError\n",
      "2. nonexist not found in the hdf5 file\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Groups.hdf5', 'r') as f:\n",
    "    \n",
    "    #Using try for handling exception\n",
    "    try:\n",
    "        f['nonexist']\n",
    "    except KeyError:\n",
    "        print('1. Caught KeyError')\n",
    "        \n",
    "    group = f.get('nonexist')\n",
    "    if group is None:\n",
    "        print('2. nonexist not found in the hdf5 file')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can also iterate through the contents of a file as we would in a python dict as we see in the following code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of keys at the root level are 2\n",
      "2. Keys at the root level are ['dset1', 'group1']\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Groups.hdf5', 'r') as f:\n",
    "    print('1. Number of keys at the root level are', len(f))\n",
    "    print('2. Keys at the root level are', [k for k in f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see we only get to see the keys at the root level and not the entire hierarchy, e.g. ``group1/group2`` and ``group1/group3`` are not seen in this list.\n",
    "\n",
    "---\n",
    "\n",
    "Lets now look at links in HDF5\n",
    "\n",
    "We will start with Hardlinks first with some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Group x name is /groupx\n",
      "2. Group x parent is <HDF5 group \"/\" (2 members)>\n",
      "3. grpx is <HDF5 group \"/groupx\" (0 members)>\n",
      "4. Group y name is /y\n",
      "5. Group y parent is <HDF5 group \"/\" (2 members)>\n",
      "6. grpy is <HDF5 group \"/y\" (0 members)>\n",
      "7. grpx is <HDF5 group \"/groupx\" (1 members)>\n",
      "8. grpy is <HDF5 group \"/y\" (1 members)>\n",
      "9. Children of grpx and grpy are ['dset'] and ['dset'] respectively\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Hardlink.hdf5', 'w') as f:\n",
    "    grpx = f.create_group('groupx')\n",
    "    f['y'] = grpx\n",
    "    grpy = f['y']\n",
    "    print('1. Group x name is', grpx.name)\n",
    "    print('2. Group x parent is', grpx.parent)\n",
    "    print('3. grpx is', grpx)\n",
    "    print('4. Group y name is', grpy.name)\n",
    "    print('5. Group y parent is', grpy.parent)\n",
    "    print('6. grpy is', grpy)\n",
    "    grpx['dset'] = np.random.random((10, 10))\n",
    "    print('7. grpx is', grpx)\n",
    "    print('8. grpy is', grpy)\n",
    "    print('9. Children of grpx and grpy are', [k for k in grpx], 'and', [k for k in grpy], 'respectively')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As we see in the above snippet, we do the following\n",
    "\n",
    "1. Create a groupx in the HDF5 file. The output prints the group name. We also create the a groupy and assign it groupx on line 4 of the code above.\n",
    "2. We print the parent of the groupx, we see it is root and has two children. The children as we know has to be  groupx and groupy.\n",
    "3. Print groupx variable. We see it has 0 children.\n",
    "4. Nect three outputs print the groupy's name, parent and ``grpy`` object to the console.\n",
    "5. As we see here the ``grpy`` object too as no children.\n",
    "6. We create a dataset under groupx called dset and assign it an array.\n",
    "7. Now we print the object ``grpx`` and ``grpy`` again. Here we see the that ``grpy`` too  has one children, even if we didn't create a dataset in it. This child of grpx is ``dset`` that we created in groupx. \n",
    "8. Since groupy is a hard link to groupx. Any changes done in either groupx or groupy will be seen in other group too as both these are just names pointing to the same address in the HDF5 file.\n",
    "\n",
    "\n",
    "Whenever we delete an object from the file and close the file. We might end up with a hold in the HDF5 file and cause the file to bloat. If we see the file consumes a lot of space we can always repack the file and compress the file system (kind of defragment of filesystem). The way to repack is done using ``h5repack`` command and the necessary command lines options supported can he found [here](https://support.hdfgroup.org/HDF5/doc/RM/Tools.html#Tools-Repack)\n",
    "\n",
    "---\n",
    "\n",
    "Similar to Hardlinks we have Softlinks in HDF5\n",
    "\n",
    "Just like a softlink on file system, the softlink points to a path and not to the actual object in the HDF5 file. Lets see a softlink in action in the following snippet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Softlink path is /group/dataset\n",
      "2. f['softlink'] is <HDF5 dataset \"softlink\": shape (3, 1), type \"<f8\">\n",
      "3. Key error expected, got one\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Softlink.hdf5', 'w') as f:\n",
    "    grp = f.create_group('group')\n",
    "    dset = grp.create_dataset('dataset', data = np.random.random((3, 1)))\n",
    "    softlink = h5py.SoftLink('/group/dataset')\n",
    "    f['softlink'] = softlink\n",
    "    print('1. Softlink path is', softlink.path)\n",
    "    print('2. f[\\'softlink\\'] is', f['softlink'])\n",
    "    grp.move('dataset', 'new_dataset')\n",
    "    try:\n",
    "        f['softlink']\n",
    "        assert False, 'Should never reach here as a KeyError is expected in previous statement'\n",
    "    except KeyError:\n",
    "        print('3. Key error expected, got one')        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Moving the dataset similar to above in case of hardlink doesnt throw a ``KeyError`` as hardlink is a pointer to the actual object in the file as we will see in the following code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. f['group/dataset'] is <HDF5 dataset \"dataset\": shape (3, 1), type \"<f8\">\n",
      "2. f['hardlink'] is <HDF5 dataset \"hardlink\": shape (3, 1), type \"<f8\">\n",
      "3. Moving /group/dataset to /group/dset\n",
      "4. f['group/dset'] is <HDF5 dataset \"dset\": shape (3, 1), type \"<f8\">\n",
      "5. f['hardlink'] is <HDF5 dataset \"hardlink\": shape (3, 1), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('Hardlink2.hdf5', 'w') as f:\n",
    "    grp = f.create_group('group')\n",
    "    dset = grp.create_dataset('dataset', data = np.random.rand(3, 1))\n",
    "    f['hardlink'] = dset\n",
    "    print('1. f[\\'group/dataset\\'] is', f['group/dataset'])\n",
    "    print('2. f[\\'hardlink\\'] is', f['hardlink'])\n",
    "    print('3. Moving /group/dataset to /group/dset')\n",
    "    f.move('/group/dataset', '/group/dset')\n",
    "    print('4. f[\\'group/dset\\'] is', f['group/dset'])\n",
    "    print('5. f[\\'hardlink\\'] is', f['hardlink'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As we see above, unlinke softlink, moving the dataset to dset didn't affect the hardlink as it points to the same object /group/dataset (and later /group/dset)  points to.\n",
    "\n",
    "There is also somthing called an External link where an object or path in a file can refer to another object or path on another file. Without giving more details lets see the external links in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. External Dataset is <HDF5 dataset \"dataset\": shape (10, 1), type \"<f8\">\n",
      "2. datasets parent is <HDF5 group \"/group\" (1 members)>\n",
      "3. datasets parent is <HDF5 file \"External.hdf5\" (mode r+)>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('External.hdf5', 'w') as f:\n",
    "    f['group/dataset'] = np.random.rand(10, 1)\n",
    "    \n",
    "with h5py.File('Source.hdf5', 'w') as f:\n",
    "    f['grp/dset'] = h5py.ExternalLink(filename= 'External.hdf5', path= 'group/dataset')\n",
    "    print('1. External Dataset is', f['grp/dset'])\n",
    "    print('2. dataset''s parent is', f['grp/dset'].parent)\n",
    "    print('3. dataset''s parent is', f['grp/dset'].file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above, grp/dset gives is a dataset which actually resides in the external file. The parent of the dataset is /group (and not /grp) which is the parent of the dataset in the external and so is the ``.file`` property which is for the file ``External.hdf5``.\n",
    "\n",
    "Rememeber the following two points though\n",
    "\n",
    "1. With external links, the external file should be accessible at the same relative location the external link is created at.\n",
    "2. With external links,  we can easily cross borders of the files and find ourselves reading data from multiple files\n",
    "\n",
    "---\n",
    "\n",
    "When opening the file in append mode we require a group to be present, but not get an error by invoking the ``create_group`` or ``create_dataset`` multiple times. In such case we have a couple or method to rescue as we see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Group is <HDF5 group \"/group\" (1 members)>\n",
      "2. Dataset is <HDF5 dataset \"dataset\": shape (10, 1), type \"<f4\">\n",
      "3. Caught ValueError as expected\n",
      "4. Group is <HDF5 group \"/group\" (1 members)>\n",
      "5. Group1 is <HDF5 group \"/group1\" (0 members)>\n",
      "6. Caught ValueError as expected with message Datatypes cannot be safely cast (existing float32 vs new float64)\n",
      "7. Caught ValueError as expected with message Shapes do not match (existing (10, 1) vs new (10, 2))\n",
      "8. dset is <HDF5 dataset \"dataset\": shape (10, 1), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "# First create a file with a dataset and a group\n",
    "with h5py.File('NewFile.hdf5', 'w') as f:\n",
    "    grp = f.create_group('group')\n",
    "    dset = grp.create_dataset('dataset', data = np.random.rand(10, 1), dtype = 'float32')\n",
    "    print('1. Group is', grp)\n",
    "    print('2. Dataset is', dset)\n",
    "    \n",
    "#Now open in r+ mode\n",
    "with h5py.File('NewFile.hdf5', 'r+') as f:\n",
    "    try:\n",
    "        f.create_group('group')\n",
    "        assert False, 'ValueError expected'\n",
    "    except ValueError:\n",
    "        print('3. Caught ValueError as expected')\n",
    "        \n",
    "    # Use require_group\n",
    "    group = f.require_group('group')\n",
    "    group1 = f.require_group('group1')\n",
    "    print('4. Group is', group)\n",
    "    print('5. Group1 is', group1)\n",
    "    \n",
    "    \n",
    "    #Similarly we have require_dataset    \n",
    "    try:\n",
    "        group.require_dataset('dataset', shape = (10, 1), dtype = 'float64')\n",
    "        assert False, 'Expected TypeError'\n",
    "    except TypeError as te:\n",
    "        print('6. Caught ValueError as expected with message', te)\n",
    "        \n",
    "    try:\n",
    "        group.require_dataset('dataset', shape = (10, 2), dtype = 'float32')\n",
    "        assert False, 'Expected TypeError'\n",
    "    except TypeError as te:\n",
    "        print('7. Caught ValueError as expected with message', te)\n",
    "    \n",
    "    dset = group.require_dataset('dataset', shape = (10, 1), dtype = 'float32')\n",
    "    print('8. dset is', dset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A slightly longisg code snippet above whows us how to use ``require_dataset`` and ``require_group``. The function ``require_dataset`` has some quirks to remember.\n",
    "\n",
    "1. the shape and dtype is **always** required to be provided. The library wants to be sure you know what you are requesting\n",
    "2. The shape has to match exactly\n",
    "3. The dtype can be something that can be casted to. For e.g, the dataset above was ``float32``, but when we requested it to be of type ``float64``, it failed. Similarly, if the dataset was of type ``int32``, casting it to ``int16`` and ``int8`` would be ok but if we try casting it to ``int64`` we will get a ``TypeError``\n",
    "\n",
    "---\n",
    "\n",
    "Lets see how to get all the groups in the HDF5 file, just like the walk ``os.walk``. HDF5 library has a special method ``visit`` on each group which accepts a callback function as we shall see in the following code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dset1\n",
      "group1\n",
      "group1/group2\n",
      "group1/group3\n",
      "group1/group3/dset2\n"
     ]
    }
   ],
   "source": [
    "def visit1(name):\n",
    "    print(name)\n",
    "    \n",
    "\n",
    "with h5py.File('Groups.hdf5', 'r') as f:\n",
    "    f.visit(visit1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above, the ``visit`` function visits various objects (datasets and groups)  in the HDF5 file in a to a [Depth First Search (DFS)](https://en.wikipedia.org/wiki/Depth-first_search) fashion. \n",
    "\n",
    "Lets look at another function ``visititems`` which gives is both the name of object and the actual object. Note that this approach is expensive as all object need to be opened and passed to the method in case. Use ``visititems`` only in case we definitely need the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dset1 <HDF5 dataset \"dset1\": shape (10, 10), type \"<f8\">\n",
      "group1 <HDF5 group \"/group1\" (2 members)>\n",
      "group1/group2 <HDF5 group \"/group1/group2\" (0 members)>\n",
      "group1/group3 <HDF5 group \"/group1/group3\" (1 members)>\n",
      "group1/group3/dset2 <HDF5 dataset \"dset2\": shape (10, 10), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "def visit2(name, obj):\n",
    "    print(name, obj)\n",
    "    \n",
    "with h5py.File('Groups.hdf5', 'r') as f:\n",
    "    f.visititems(visit2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lets look at a way to use the inexpensive ``visit`` method and yet have a possibility of accessing the object if required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dset1 <HDF5 dataset \"dset1\": shape (10, 10), type \"<f8\">\n",
      "group1 <HDF5 group \"/group1\" (2 members)>\n",
      "group1/group2 <HDF5 group \"/group1/group2\" (0 members)>\n",
      "group1/group3 <HDF5 group \"/group1/group3\" (1 members)>\n",
      "group1/group3/dset2 <HDF5 dataset \"dset2\": shape (10, 10), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "#We will use the partial functions from functional programming paradigm and pass a partial function\n",
    "from functools import partial\n",
    "\n",
    "def visit3(grp, name):\n",
    "    #This is no different than visititems as we open all objects, this can be conditional for the object names\n",
    "    #of our interest\n",
    "    print(name, grp[name])\n",
    "\n",
    "with h5py.File('Groups.hdf5', 'r') as f:\n",
    "    pfunc = partial(visit3, f)\n",
    "    f.visit(pfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One final bit for visiting the nodes, in all cases above, the function didnt return anything (return ``None``), which is why the ``visit`` traversed the entire HDF5 file. Suppose we want to terminate the traversal midway when we have found our object of interest, we simply return an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched, object, name pair is (<HDF5 dataset \"dset2\": shape (10, 10), type \"<f8\">, 'group1/group3/dset2')\n"
     ]
    }
   ],
   "source": [
    "def visit4(grp, name):\n",
    "    if name[-5:] == 'dset2':\n",
    "        #We return an object tuple. Any object other than a None, when returned will be returned by the\n",
    "        #visit function and further traversal of the object/object names will stop\n",
    "        return (grp[name], name)\n",
    "\n",
    "with h5py.File('Groups.hdf5', 'r') as f:\n",
    "    pfunc = partial(visit4, f)\n",
    "    obj = f.visit(pfunc)\n",
    "    print('Matched, object, name pair is', obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Storing Attributes and Metadata\n",
    "\n",
    "We now will look at storing attributes and metadata along with a group in HDF5 file.\n",
    "Attributes are a good way to store metadata along with the groups/datasets. This makes the content self describing.\n",
    "\n",
    "Following code snippet demonstrates how we can attach, access and iterate through the attributes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Attributes for dataset dset are ['date', 'version', 'author']\n",
      "2. Attributes for dataset dset are [('date', '5Jan2018'), ('version', 1), ('author', 'Amol')]\n",
      "3. Caught KeyError \"Can't open attribute (Can't locate attribute: 'nonexist')\"\n",
      "4. attrval is None\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('AttribTest.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset('dset', data = np.random.rand(10, 10), dtype = 'float32')\n",
    "    dset.attrs['date'] = '5Jan2018'\n",
    "    dset.attrs['version'] = 1\n",
    "    dset.attrs['author'] = 'Amol'\n",
    "    \n",
    "    #1. Get all arribute names\n",
    "    print('1. Attributes for dataset dset are', [a for a in dset.attrs])\n",
    "    \n",
    "    #2. Get all key value pairs\n",
    "    print('2. Attributes for dataset dset are', [(a, av) for (a, av) in dset.attrs.items()])\n",
    "    \n",
    "    #3. Access non existent attribute\n",
    "    try:\n",
    "        dset.attrs['nonexist']\n",
    "        assert False, 'Exception expected'\n",
    "    except KeyError as err:\n",
    "        print('3. Caught KeyError', err)\n",
    "        \n",
    "    #4. Use get to retrieve non existent keys without exception\n",
    "    attrval = dset.attrs.get('nonexist')\n",
    "    if attrval is None:\n",
    "        print('4. attrval is None')\n",
    "    else:\n",
    "        assert False, 'Expected a None for a non existent attribute'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "With default settings, the size of the HDF5 attribute is limited to 64kb. There are few quirks though especially when we try to overwrite an existing attribute with a value that exceeds 64kb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Existing attribute value for attribute attr is original\n",
      "2. Caught RuntimeError Unable to create attribute (Object header message is too large)\n",
      "3. Caught AttributeError, looks like we even lost the original attribute value\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('LongAttrib.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset('dset', shape = (10, 1))\n",
    "    dset.attrs['attr'] = 'original'\n",
    "    print('1. Existing attribute value for attribute attr is', dset.attrs['attr'])\n",
    "    try:\n",
    "        dset.attrs['attr'] = np.ones((100, 100))\n",
    "        assert False\n",
    "    except RuntimeError as err:\n",
    "        print('2. Caught RuntimeError', err)\n",
    "    \n",
    "    try:\n",
    "        dset.attr['attr']\n",
    "        assert False\n",
    "    except AttributeError:\n",
    "        print('3. Caught AttributeError, looks like we even lost the original attribute value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As we see above, the attribute setting operation is not atomic. When we tried to set an attribute with value larger than the permitted size of 64kb, an error was thrown. However, as we see next, even the value we set originally is lost which shows this attribute setting isn't an atomic operation and we need to exercise caution while setting large attributes. A work around to this is to set attribute as a reference to another dataset as we see below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Attribute now is <HDF5 object reference>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('LongAttrib.hdf5', 'r+') as f:\n",
    "    f['attr_dset'] = np.ones((100, 100))\n",
    "    f['dset'].attrs['attr'] = f['attr_dset'].ref\n",
    "    print('1. Attribute now is', f['dset'].attrs['attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
